{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efbb14c",
   "metadata": {},
   "source": [
    "### Clelia Caetano 2023060 (CA2_SEM2)\n",
    "### MSc. in Data Analytics\n",
    "### Project Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb91c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8b8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334b3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073afae",
   "metadata": {},
   "source": [
    "### Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8bdecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session with necessary configurations\n",
    "spark = SparkSession.builder.appName('ProjectTweets') \\\n",
    "                    .config(\"spark.some_config_option\", \"config_value\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37eab8",
   "metadata": {},
   "source": [
    "### Read the Data from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739e385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+-------------+--------------------+\n",
      "|number|  id_tweet|                date|   query|      user_id|               tweet|\n",
      "+------+----------+--------------------+--------+-------------+--------------------+\n",
      "|     1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|scotthamilton|is upset that he ...|\n",
      "|     2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|     mattycus|@Kenichan I dived...|\n",
      "|     3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|      ElleCTF|my whole body fee...|\n",
      "|     4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|       Karoli|@nationwideclass ...|\n",
      "|     5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|     joy_wolf|@Kwesidei not the...|\n",
      "+------+----------+--------------------+--------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data and create a temporary view in Spark\n",
    "try:\n",
    "    # Load the data.csv file\n",
    "    data = spark.read.csv(\"file:///home/hduser/Desktop/CA2_SEM2/ProjectTweets.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Check if the data was loaded successfully\n",
    "    if data is not None:\n",
    "        # Define column names\n",
    "        new_column_names = [\"number\", \"id_tweet\", \"date\", \"query\", \"user_id\", \"tweet\"]\n",
    "\n",
    "        # Use the alias method to rename the columns\n",
    "        for i in range(len(new_column_names)):\n",
    "            data = data.withColumnRenamed(data.columns[i], new_column_names[i])\n",
    "\n",
    "        # Create a temporary table from the DataFrame\n",
    "        data.createOrReplaceTempView(\"CA2_ProjectTweets\")\n",
    "\n",
    "        # Run Spark SQL queries using the same SparkSession\n",
    "        data = spark.sql(\"SELECT * FROM CA2_ProjectTweets\") \n",
    "\n",
    "        # Display the first 5 rows\n",
    "        data.show(5)\n",
    "    else:\n",
    "        print(\"Data not loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33efac",
   "metadata": {},
   "source": [
    "### Create a Hive database in Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfe5cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 14:52:46,800 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-25 14:52:46,802 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-25 14:52:49,048 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "2023-10-25 14:52:49,050 WARN metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hduser@127.0.1.1\n",
      "2023-10-25 14:52:49,526 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "2023-10-25 14:52:55,165 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "2023-10-25 14:52:55,227 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "2023-10-25 14:52:55,228 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-25 14:52:55,228 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a Hive table\n",
    "data.write.mode(\"overwrite\").saveAsTable(\"ca2_projecttweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "697d6d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the current database\n",
    "spark.sql(\"USE ca2_projecttweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60af3923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+-------------+--------------------+\n",
      "|number|  id_tweet|                date|   query|      user_id|               tweet|\n",
      "+------+----------+--------------------+--------+-------------+--------------------+\n",
      "|     1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|scotthamilton|is upset that he ...|\n",
      "|     2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|     mattycus|@Kenichan I dived...|\n",
      "|     3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|      ElleCTF|my whole body fee...|\n",
      "|     4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|       Karoli|@nationwideclass ...|\n",
      "|     5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|     joy_wolf|@Kwesidei not the...|\n",
      "+------+----------+--------------------+--------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a SQL query to select all rows from the table\n",
    "result = spark.sql(\"SELECT `number`, `id_tweet`, `date`, `query`, `user_id`, `tweet` FROM ca2_projecttweets\")\n",
    "\n",
    "# Show the first 5 rows\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3ea44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed data as a new table if needed\n",
    "result.write.mode(\"overwrite\").saveAsTable(\"preprocessed_ca2_projecttweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "811dcc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: integer (nullable = true)\n",
      " |-- id_tweet: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access the Hive table and create a DataFrame\n",
    "data = spark.table(\"preprocessed_ca2_projecttweets\")\n",
    "\n",
    "# Print the schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df25ce67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is: 1599999 rows\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the data\n",
    "data_size = data.count()\n",
    "print(f\"The size of the dataset is: {data_size} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb792364",
   "metadata": {},
   "source": [
    "### Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f29f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall unique value counts:\n",
      "number: 1599999\n",
      "id_tweet: 1598314\n",
      "date: 774362\n",
      "query: 1\n",
      "user_id: 659775\n",
      "tweet: 1581465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique values in each column.\n",
    "# List of column names\n",
    "column_names = [\"number\", \"id_tweet\", \"date\", \"query\", \"user_id\", \"tweet\"]\n",
    "\n",
    "# Dictionary to store unique value counts\n",
    "unique_counts = {}\n",
    "\n",
    "# Iterate through the columns and count unique values\n",
    "for column_name in column_names:\n",
    "    unique_values = data.select(column_name).distinct()\n",
    "    unique_count = unique_values.count()\n",
    "    unique_counts[column_name] = unique_count\n",
    "\n",
    "# Print the overall count for each column\n",
    "print(\"Overall unique value counts:\")\n",
    "for column, count in unique_counts.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec1a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                date|               tweet|\n",
      "+--------------------+--------------------+\n",
      "|Mon Apr 06 22:19:...|is upset that he ...|\n",
      "|Mon Apr 06 22:19:...|@Kenichan I dived...|\n",
      "|Mon Apr 06 22:19:...|my whole body fee...|\n",
      "|Mon Apr 06 22:19:...|@nationwideclass ...|\n",
      "|Mon Apr 06 22:20:...|@Kwesidei not the...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that are duplicates and other irrelevant data information.\n",
    "data = data.drop('number', 'id_tweet', 'query', 'user_id')\n",
    "\n",
    "# Show the updated DataFrame\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e0ed3",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61b25cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'date': 0\n",
      "Missing values in 'tweet': 0\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all columns and print missing value counts\n",
    "for column_name in data.columns:\n",
    "    missing_count = data.filter(data[column_name].isNull()).count()\n",
    "    print(f\"Missing values in '{column_name}': {missing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d7bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37de73",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3c0af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|                date|               tweet|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|             1599999|             1599999|\n",
      "|   mean|                null|                null|\n",
      "| stddev|                null|                null|\n",
      "|    min|Fri Apr 17 20:30:...|                 ...|\n",
      "|    max|Wed May 27 07:27:...|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute and interpret the mean, median, quartiles and standard deviation of the dataset\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e98a3",
   "metadata": {},
   "source": [
    "### Create a configuration to convert string 'date' into timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a454c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2009-04-07 06:19:49|\n",
      "|2009-04-07 06:19:53|\n",
      "|2009-04-07 06:19:57|\n",
      "|2009-04-07 06:19:57|\n",
      "|2009-04-07 06:20:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession with the 'spark.sql.legacy.timeParserPolicy' configuration\n",
    "spark = SparkSession.builder.config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
    "\n",
    "# Import libraries to convert\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Convert 'date' column in string format to the timestamp\n",
    "data = data.withColumn(\"date\", from_unixtime(unix_timestamp(data[\"date\"], \"E MMM dd HH:mm:ss z yyyy\")).cast(TimestampType()))\n",
    "\n",
    "# Print the 'date' column with the new format\n",
    "data.select(\"date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef2d265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Date: 2009-04-07\n",
      "Last Date: 2009-06-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-04-08\n",
      "2009-04-09\n",
      "2009-04-10\n",
      "2009-04-11\n",
      "2009-04-12\n",
      "2009-04-13\n",
      "2009-04-14\n",
      "2009-04-15\n",
      "2009-04-16\n",
      "2009-04-17\n",
      "2009-04-22\n",
      "2009-04-23\n",
      "2009-04-24\n",
      "2009-04-25\n",
      "2009-04-26\n",
      "2009-04-27\n",
      "2009-04-28\n",
      "2009-04-29\n",
      "2009-04-30\n",
      "2009-05-01\n",
      "2009-05-05\n",
      "2009-05-06\n",
      "2009-05-07\n",
      "2009-05-08\n",
      "2009-05-09\n",
      "2009-05-13\n",
      "2009-05-15\n",
      "2009-05-16\n",
      "2009-05-19\n",
      "2009-05-20\n",
      "2009-05-21\n",
      "2009-05-23\n",
      "2009-05-26\n",
      "2009-05-28\n",
      "2009-06-09\n",
      "2009-06-10\n",
      "2009-06-11\n",
      "2009-06-12\n",
      "2009-06-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_add, datediff\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Extract the date part from the 'date' column\n",
    "data = data.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "data = data.orderBy(\"date\")\n",
    "\n",
    "# Calculate the next date using the lead function\n",
    "data = data.withColumn(\"next_date\", date_add(col(\"date\"), 1))\n",
    "\n",
    "# Check if the next date is continuous (no break)\n",
    "data = data.withColumn(\"is_continuous\", datediff(col(\"next_date\"), col(\"date\")) == 1)\n",
    "\n",
    "# Create a window specification to order by date\n",
    "window_spec = Window.orderBy(\"date\")\n",
    "\n",
    "# Assign a group identifier to continuous sequences\n",
    "data = data.withColumn(\"group_id\", F.sum(col(\"is_continuous\").cast(\"int\")).over(window_spec))\n",
    "\n",
    "# Filter only the rows with continuous sequences\n",
    "continuous_dates = data.filter(col(\"is_continuous\")).select(\"date\").distinct()\n",
    "\n",
    "# Sort the list in ascending order\n",
    "continuous_dates = continuous_dates.orderBy(\"date\")\n",
    "\n",
    "# Get the first and last dates in the DataFrame\n",
    "first_date = continuous_dates.first().date\n",
    "last_date = continuous_dates.orderBy(col(\"date\").desc()).first().date\n",
    "\n",
    "# Print the first and last dates\n",
    "print(\"First Date:\", first_date)\n",
    "print(\"Last Date:\", last_date)\n",
    "\n",
    "# Define the start and end date for the range\n",
    "start_date = first_date\n",
    "end_date = last_date\n",
    "\n",
    "# Generate a list of all dates in the range\n",
    "date_range = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Convert the list of continuous dates to a Python list\n",
    "continuous_dates_list = [row.date for row in continuous_dates.collect()]\n",
    "\n",
    "# Find the dates not in the list\n",
    "missing_dates = [d for d in date_range if d not in continuous_dates_list]\n",
    "\n",
    "# Print the missing dates\n",
    "for missing_date in missing_dates:\n",
    "    print(missing_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5befb8",
   "metadata": {},
   "source": [
    "### Create a HBase database in Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd9b3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install happybase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841a4c0",
   "metadata": {},
   "source": [
    "import happybase\n",
    "\n",
    "# Specify your HBase host and port\n",
    "hbase_host = \"localhost\"  \n",
    "hbase_port = 9095  \n",
    "\n",
    "# Initialize the HappyBase connection\n",
    "connection = happybase.Connection(host=hbase_host, port=hbase_port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972ac05",
   "metadata": {},
   "source": [
    "# Define the HBase configuration\n",
    "hbaseConfig = {\n",
    "    \"hbase.zookeeper.quorum\": \"your_zookeeper_quorum\",\n",
    "    \"hbase.zookeeper.property.clientPort\": \"9095\",\n",
    "    \"hbase.client.keyvalue.maxsize\": \"10485760\"\n",
    "}\n",
    "\n",
    "# Define the HBase table name and column family\n",
    "tableName = \"YourHBaseTableName\"\n",
    "columnFamily = \"cf\"  # Change this to your desired column family\n",
    "\n",
    "# Write the data from the DataFrame to HBase\n",
    "data.write.format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .option(\"hbase.columns.mapping\", \"id_tweet:cf:id_tweet,date:cf:date,query:cf:query,user_id:cf:user_id,tweet:cf:tweet\") \\\n",
    "    .option(\"hbase.table\", tableName) \\\n",
    "    .option(\"hbase.zookeeper.quorum\", hbaseConfig[\"hbase.zookeeper.quorum\"]) \\\n",
    "    .option(\"hbase.zookeeper.property.clientPort\", hbaseConfig[\"hbase.zookeeper.property.clientPort\"]) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6b269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
