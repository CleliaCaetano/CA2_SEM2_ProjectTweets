{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efbb14c",
   "metadata": {},
   "source": [
    "### Clelia Caetano 2023060 (CA2_SEM2)\n",
    "### MSc. in Data Analytics\n",
    "### Project Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb91c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8b8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334b3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073afae",
   "metadata": {},
   "source": [
    "### Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8bdecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session with necessary configurations\n",
    "spark = SparkSession.builder.appName('ProjectTweets') \\\n",
    "                    .config(\"spark.some_config_option\", \"config_value\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37eab8",
   "metadata": {},
   "source": [
    "### Read the Data from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739e385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|number|  id_tweet|                date|   query|        user_id|               tweet|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|     1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|     2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|     3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|     4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|     5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|     6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|     7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|     8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|     9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "|    10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "|    11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "|    12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "|    13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "|    14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "|    15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "|    16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "|    17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "|    18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "|    19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "|    20|1467813985|Mon Apr 06 22:20:...|NO_QUERY|         quanvu|@alydesigns i was...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data and create a temporary view in Spark\n",
    "try:\n",
    "    # Load the data.csv file\n",
    "    data = spark.read.csv(\"file:///home/hduser/Desktop/CA2_SEM2/ProjectTweets.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Check if the data was loaded successfully\n",
    "    if data is not None:\n",
    "        # Define column names\n",
    "        new_column_names = [\"number\", \"id_tweet\", \"date\", \"query\", \"user_id\", \"tweet\"]\n",
    "\n",
    "        # Use the alias method to rename the columns\n",
    "        for i in range(len(new_column_names)):\n",
    "            data = data.withColumnRenamed(data.columns[i], new_column_names[i])\n",
    "\n",
    "        # Create a temporary table from the DataFrame\n",
    "        data.createOrReplaceTempView(\"CA2_ProjectTweets\")\n",
    "\n",
    "        # Run Spark SQL queries using the same SparkSession\n",
    "        data = spark.sql(\"SELECT * FROM CA2_ProjectTweets\") \n",
    "\n",
    "        # Display the first 20 rows\n",
    "        data.show()\n",
    "    else:\n",
    "        print(\"Data not loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33efac",
   "metadata": {},
   "source": [
    "### Create a Hive database in Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfe5cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 13:44:50,916 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-25 13:44:50,917 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-25 13:44:53,351 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "2023-10-25 13:44:53,352 WARN metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hduser@127.0.1.1\n",
      "2023-10-25 13:44:54,030 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "2023-10-25 13:44:59,711 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "2023-10-25 13:44:59,785 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "2023-10-25 13:44:59,785 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-25 13:44:59,785 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a Hive table\n",
    "data.write.mode(\"overwrite\").saveAsTable(\"ca2_projecttweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "697d6d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the current database\n",
    "spark.sql(\"USE ca2_projecttweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60af3923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+-------------+--------------------+\n",
      "|number|  id_tweet|                date|   query|      user_id|               tweet|\n",
      "+------+----------+--------------------+--------+-------------+--------------------+\n",
      "|     1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|scotthamilton|is upset that he ...|\n",
      "|     2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|     mattycus|@Kenichan I dived...|\n",
      "|     3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|      ElleCTF|my whole body fee...|\n",
      "|     4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|       Karoli|@nationwideclass ...|\n",
      "|     5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|     joy_wolf|@Kwesidei not the...|\n",
      "+------+----------+--------------------+--------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a SQL query to select all rows from the table\n",
    "result = spark.sql(\"SELECT `number`, `id_tweet`, `date`, `query`, `user_id`, `tweet` FROM ca2_projecttweets\")\n",
    "\n",
    "# Show the first 5 rows\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3ea44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed data as a new table if needed\n",
    "result.write.mode(\"overwrite\").saveAsTable(\"preprocessed_ca2_projecttweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "811dcc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: integer (nullable = true)\n",
      " |-- id_tweet: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access the Hive table and create a DataFrame\n",
    "data = spark.table(\"preprocessed_ca2_projecttweets\")\n",
    "\n",
    "# Print the schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df25ce67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is: 1599999 rows\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the data\n",
    "data_size = data.count()\n",
    "print(f\"The size of the dataset is: {data_size} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb792364",
   "metadata": {},
   "source": [
    "### Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f29f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall unique value counts:\n",
      "number: 1599999\n",
      "id_tweet: 1598314\n",
      "date: 774362\n",
      "query: 1\n",
      "user_id: 659775\n",
      "tweet: 1581465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique values in each column.\n",
    "# List of column names\n",
    "column_names = [\"number\", \"id_tweet\", \"date\", \"query\", \"user_id\", \"tweet\"]\n",
    "\n",
    "# Dictionary to store unique value counts\n",
    "unique_counts = {}\n",
    "\n",
    "# Iterate through the columns and count unique values\n",
    "for column_name in column_names:\n",
    "    unique_values = data.select(column_name).distinct()\n",
    "    unique_count = unique_values.count()\n",
    "    unique_counts[column_name] = unique_count\n",
    "\n",
    "# Print the overall count for each column\n",
    "print(\"Overall unique value counts:\")\n",
    "for column, count in unique_counts.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec1a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                date|               tweet|\n",
      "+--------------------+--------------------+\n",
      "|Mon Apr 06 22:19:...|is upset that he ...|\n",
      "|Mon Apr 06 22:19:...|@Kenichan I dived...|\n",
      "|Mon Apr 06 22:19:...|my whole body fee...|\n",
      "|Mon Apr 06 22:19:...|@nationwideclass ...|\n",
      "|Mon Apr 06 22:20:...|@Kwesidei not the...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that are duplicates and other irrelevant data information.\n",
    "data = data.drop('number', 'id_tweet', 'query', 'user_id')\n",
    "\n",
    "# Show the updated DataFrame\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e14de",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61b25cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'date': 0\n",
      "Missing values in 'tweet': 0\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all columns and print missing value counts\n",
    "for column_name in data.columns:\n",
    "    missing_count = data.filter(data[column_name].isNull()).count()\n",
    "    print(f\"Missing values in '{column_name}': {missing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d7bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f1454",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eba9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|                date|               tweet|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|             1599999|             1599999|\n",
      "|   mean|                null|                null|\n",
      "| stddev|                null|                null|\n",
      "|    min|Fri Apr 17 20:30:...|                 ...|\n",
      "|    max|Wed May 27 07:27:...|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute and interpret the mean, median, quartiles and standard deviation of the dataset\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8892ee",
   "metadata": {},
   "source": [
    "### Create a configuration to convert string 'date' into timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf415894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2009-04-07 06:19:49|\n",
      "|2009-04-07 06:19:53|\n",
      "|2009-04-07 06:19:57|\n",
      "|2009-04-07 06:19:57|\n",
      "|2009-04-07 06:20:00|\n",
      "|2009-04-07 06:20:03|\n",
      "|2009-04-07 06:20:03|\n",
      "|2009-04-07 06:20:05|\n",
      "|2009-04-07 06:20:09|\n",
      "|2009-04-07 06:20:16|\n",
      "|2009-04-07 06:20:17|\n",
      "|2009-04-07 06:20:19|\n",
      "|2009-04-07 06:20:19|\n",
      "|2009-04-07 06:20:20|\n",
      "|2009-04-07 06:20:20|\n",
      "|2009-04-07 06:20:22|\n",
      "|2009-04-07 06:20:25|\n",
      "|2009-04-07 06:20:31|\n",
      "|2009-04-07 06:20:34|\n",
      "|2009-04-07 06:20:37|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession with the 'spark.sql.legacy.timeParserPolicy' configuration\n",
    "spark = SparkSession.builder.config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
    "\n",
    "# Import libraries to convert\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Convert 'date' column in string format to the timestamp\n",
    "data = data.withColumn(\"date\", from_unixtime(unix_timestamp(data[\"date\"], \"E MMM dd HH:mm:ss z yyyy\")).cast(TimestampType()))\n",
    "\n",
    "# Include the \"date\" column in the DataFrame\n",
    "data = data.select(\"date\", \"*\")\n",
    "\n",
    "# Print the 'date' column with the new format\n",
    "data.select(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9ed74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0eae2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b5befb8",
   "metadata": {},
   "source": [
    "### Create a HBase database in Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install happybase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d616b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase\n",
    "\n",
    "# Specify your HBase host and port\n",
    "hbase_host = \"localhost\"  # Replace with your HBase host\n",
    "hbase_port = 9095  # Replace with your HBase port\n",
    "\n",
    "# Initialize the HappyBase connection\n",
    "connection = happybase.Connection(host=hbase_host, port=hbase_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ca56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
